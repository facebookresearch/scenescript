# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import torch
import torchsparse
import torchsparse.nn as spnn
from einops import repeat

from torch import nn
from torch.nn import functional as F


def make_conv3d_sparse(
    channels_in,
    channels_out,
    kernel_size=3,
    num_groups=8,
    activation=spnn.ReLU,
):
    num_groups = min(num_groups, channels_out)
    block = nn.Sequential(
        spnn.Conv3d(channels_in, channels_out, kernel_size=kernel_size, stride=1),
        spnn.GroupNorm(num_groups, channels_out),
        activation(inplace=True),
    )
    return block


def make_conv3d_downscale_sparse(
    channels_in,
    channels_out,
    num_groups=8,
    activation=spnn.ReLU,
):
    num_groups = min(num_groups, channels_out)
    block = nn.Sequential(
        spnn.Conv3d(channels_in, channels_out, kernel_size=2, stride=2),
        spnn.GroupNorm(num_groups, channels_out),
        activation(inplace=True),
    )
    return block


class ResBlockSparse(nn.Module):
    def __init__(
        self,
        channels,
        num_groups=8,
        activation=spnn.ReLU,
    ):
        super().__init__()

        self.block0 = make_conv3d_sparse(
            channels, channels, num_groups=num_groups, activation=activation
        )
        self.block1 = make_conv3d_sparse(
            channels, channels, num_groups=num_groups, activation=activation
        )

    def forward(self, x):
        out = self.block0(x)
        out = self.block1(out)
        return x + out


def index_batched_sparse_tensor(sparse_tensor, index):
    """Index into a batched torchsparse.SparseTensor.

    Args:
        sparse_tensor: a torchsparse.SparseTensor that is the output of
            torchsparse.utils.collate.sparse_collate().
        index: int.

    Returns:
        torchsparse.SparseTensor with no batch dimension.
    """
    batch_mask = sparse_tensor.C[:, 0] == index
    coords = sparse_tensor.C[batch_mask, 1:]  # Get rid of batch dim
    feats = sparse_tensor.F[batch_mask]
    return torchsparse.SparseTensor(
        coords=coords,
        feats=feats,
        stride=sparse_tensor.s,
    )


def sparse_uncollate(sparse_tensor):
    """Un-Collate a batched torchsparse.SparseTensor.

    Args:
        sparse_tensor: a torchsparse.SparseTensor that is the output of
            torchsparse.utils.collate.sparse_collate().

    Returns:
        List[torchsparse.SparseTensor].
    """
    batch_size = sparse_tensor.C[:, 0].max() + 1

    sparse_tensor_list = []
    for b in range(batch_size):
        sparse_tensor_list.append(index_batched_sparse_tensor(sparse_tensor, b))
    return sparse_tensor_list


def vox_to_sequence(sparse_tensor):
    """Compute sequence from sparse point cloud.

    Args:
        sparse_tensor: torchsparse.SparseTensor.

    Returns:
        Dict with the following keys:
            seq: [B, maxlen, C] torch.FloatTensor.
            coords: [B, maxlen, 3] torch.IntTensor. To be used with embeddings for Transformers.
            mask: [B, maxlen] torch.BoolTensor. To be used with Transformers.
    """
    sparse_tensor_list = sparse_uncollate(sparse_tensor)
    batch_size = len(sparse_tensor_list)
    channels = sparse_tensor_list[0].F.shape[-1]
    maxlen = max([x.C.shape[0] for x in sparse_tensor_list])

    seq = torch.zeros(
        (batch_size, maxlen, channels),
        dtype=sparse_tensor.F.dtype,
        device=sparse_tensor.F.device,
    )
    coords = torch.zeros(
        (batch_size, maxlen, 3),
        dtype=sparse_tensor.C.dtype,
        device=sparse_tensor.C.device,
    )
    mask = torch.ones(
        (batch_size, maxlen),
        dtype=torch.bool,
        device=sparse_tensor.F.device,
    )
    for i in range(batch_size):
        sparse_tensor = sparse_tensor_list[i]

        # Get coords (divided by stride, so its in {0, 1, ...})
        coords_i = sparse_tensor.C  # [N_points_i, 3]
        coords_num = coords_i.shape[0]
        assert coords_num <= maxlen, f"coords_num: {coords_num} is too high..."

        # Get features
        feats = sparse_tensor.F  # [N_points_i, C]

        # Pad and save
        pad = maxlen - coords_num

        feats = F.pad(feats.T, (0, pad), value=0).T  # [maxlen, C]
        seq[i] = feats

        coords_i = F.pad(coords_i.T, (0, pad), value=0).T  # [maxlen, 3]
        coords[i] = coords_i

        mask[i, :coords_num] = False

    return {
        "seq": seq,
        "coords": coords,
        "mask": mask,
    }


def fourier_encode_vector(vec, num_bands=10, sample_rate=60):
    """Fourier encode a vector.

    Args:
        vec: [B, N, D] torch.FloatTensor.
        num_bands: int.
        sample_rate: int.

    Returns:
        [B, N, (2 * num_bands + 1) * D] torch.FloatTensor.
    """
    b, n, d = vec.shape
    samples = torch.linspace(1, sample_rate / 2, num_bands).to(vec.device) * torch.pi
    sines = torch.sin(samples[None, None, :, None] * vec[:, :, None, :])
    cosines = torch.cos(samples[None, None, :, None] * vec[:, :, None, :])

    encoding = torch.stack([sines, cosines], dim=3).reshape(b, n, 2 * num_bands, d)
    encoding = torch.cat([vec[:, :, None, :], encoding], dim=2)
    return encoding.flatten(2)


class ResNet3DSparse(nn.Module):
    def __init__(self, dim_in, dim_out, layers):
        super().__init__()

        self.stem = nn.Sequential(
            make_conv3d_sparse(dim_in, layers[0], kernel_size=7),
            ResBlockSparse(layers[0]),
        )

        # Number of down-convs is len(layers) - 1
        blocks = []
        for i in range(len(layers) - 1):
            blocks.append(
                nn.Sequential(
                    make_conv3d_downscale_sparse(layers[i], layers[i + 1]),
                    ResBlockSparse(layers[i + 1]),
                    ResBlockSparse(layers[i + 1]),
                )
            )
        self.blocks = nn.Sequential(*blocks)

        self.bottleneck = nn.Sequential(
            nn.Linear(layers[-1], 2 * layers[-1]),
            nn.GroupNorm(8, 2 * layers[-1]),  # 8 is default num_groups for GroupNorm
            nn.ReLU(inplace=True),
            nn.Linear(2 * layers[-1], dim_out),
        )

    def forward(self, x):
        out = self.stem(x)
        out = self.blocks(out)
        out.F = self.bottleneck(out.F)  # bottleneck applied to features only
        return out


class PointCloudEncoder(nn.Module):
    def __init__(
        self,
        input_channels,
        d_model,
        conv_layers,
        num_bins,
    ):
        """Point Cloud Encoder.

        Args:
            input_channels: int.
            d_model: int.
            conv_layers: List[int].
            num_bins: int.
        """

        super().__init__()

        self.sparse_resnet = ResNet3DSparse(
            dim_in=input_channels,
            dim_out=d_model,
            layers=conv_layers,
        )
        downconvs = len(conv_layers) - 1
        res_reduction = 2**downconvs  # voxel resolution reduction
        self.reduced_grid_size = int(num_bins / res_reduction)
        self.input_proj = nn.Linear(d_model + 63, d_model)  # 63 for fourier encoding

        # the following is a legacy parameter
        self.extra_embedding = nn.Parameter(torch.empty(d_model).normal_(std=0.02))

    def forward(self, point_cloud: torchsparse.SparseTensor):
        """Forward function.

        Args:
            point_cloud: torchsparse.SparseTensor.

        Returns: a Dict with the following keys:
            context: [B, maxlen, d_model] torch.FloatTensor.
            context_mask: [B, maxlen] torch.BoolTensor. True means ignore.
        """
        outputs = self.sparse_resnet(point_cloud)
        outputs = vox_to_sequence(outputs)

        context = outputs["seq"]
        context_mask = outputs["mask"]

        coords = outputs["coords"]
        coords_normalised = coords / (self.reduced_grid_size - 1)
        encoded_coords = fourier_encode_vector(coords_normalised)

        context = torch.cat([context, encoded_coords], dim=-1)
        context = self.input_proj(context)

        # legacy parameter
        context = context + repeat(self.extra_embedding, "d -> 1 1 d")

        return {
            "context": context,
            "context_mask": context_mask,
        }
